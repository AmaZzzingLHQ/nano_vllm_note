import torch
from transformers import Qwen3Config
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# ç›´æ¥ä½¿ç”¨æ¨¡æ‹Ÿç±»ï¼Œé¿å…åˆ†å¸ƒå¼åˆå§‹åŒ–
print("ä½¿ç”¨æœ¬åœ°æ¨¡æ‹Ÿç±»å±•ç¤ºQwen3æ¨¡å‹ç»“æ„...")

# åˆ›å»ºæ¨¡æ‹Ÿç±»ç”¨äºå±•ç¤ºç»“æ„
class MockAttention:
    def __init__(self, num_heads, head_dim, scale, num_kv_heads):
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.scale = scale
        self.num_kv_heads = num_kv_heads
        self.k_cache = torch.tensor([])  # æ¨¡æ‹ŸKVç¼“å­˜
        self.v_cache = torch.tensor([])

class MockQwen3Attention:
    def __init__(self, hidden_size, num_heads, num_kv_heads, **kwargs):
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.num_kv_heads = num_kv_heads
        self.attn = MockAttention(num_heads, hidden_size//num_heads, (hidden_size//num_heads)**-0.5, num_kv_heads)

class MockQwen3DecoderLayer:
    def __init__(self, config):
        self.config = config
        self.self_attn = MockQwen3Attention(
            hidden_size=config.hidden_size,
            num_heads=config.num_attention_heads,
            num_kv_heads=config.num_key_value_heads
        )
        # æ·»åŠ æ›´å¤šå±‚çš„è¯¦ç»†ä¿¡æ¯
        class MockLayerNorm:
            def __init__(self, dim, eps=1e-6):
                self.dim = dim
                self.eps = eps
        
        self.input_layernorm = MockLayerNorm(config.hidden_size)
        self.post_attention_layernorm = MockLayerNorm(config.hidden_size)
        
        class MockMLP:
            def __init__(self, hidden_size, intermediate_size):
                self.hidden_size = hidden_size
                self.intermediate_size = intermediate_size
        
        self.mlp = MockMLP(config.hidden_size, config.intermediate_size)

class Qwen3Model:
    def __init__(self, config):
        self.config = config
        # ä½¿ç”¨æ™®é€šåˆ—è¡¨è€Œä¸æ˜¯ModuleListï¼Œé¿å…ç»§æ‰¿é—®é¢˜
        self.layers = [MockQwen3DecoderLayer(config) for _ in range(config.num_hidden_layers)]
        
        # æ·»åŠ åµŒå…¥å±‚å’Œè¾“å‡ºå±‚
        class MockEmbedding:
            def __init__(self, vocab_size, hidden_size):
                self.vocab_size = vocab_size
                self.hidden_size = hidden_size
        
        self.embed_tokens = MockEmbedding(config.vocab_size, config.hidden_size)
        
        class MockRMSNorm:
            def __init__(self, dim, eps=1e-6):
                self.dim = dim
                self.eps = eps
        
        self.norm = MockRMSNorm(config.hidden_size)
    
    # æ·»åŠ named_childrenæ–¹æ³•ï¼Œæ¨¡æ‹ŸPyTorchæ¨¡å—çš„è¡Œä¸º
    def named_children(self):
        yield ('embed_tokens', self.embed_tokens)
        yield ('layers', self.layers)
        yield ('norm', self.norm)


def print_model_structure(model, prefix=''):
    """æ‰“å°æ¨¡å‹ç»“æ„ï¼Œæ ‡è®°ä½¿ç”¨KVç¼“å­˜çš„å±‚ï¼Œä¼˜åŒ–è¾“å‡ºä»¥å‡å°‘é‡å¤"""
    # å°è¯•è·å–å­æ¨¡å—ï¼Œå¦‚æœæ˜¯åˆ—è¡¨åˆ™ç‰¹æ®Šå¤„ç†
    try:
        module_list = list(model.named_children())
    except:
        # å¦‚æœæ²¡æœ‰named_childrenæ–¹æ³•ï¼Œå°è¯•ç›´æ¥è®¿é—®å±æ€§
        module_list = []
        if hasattr(model, '__dict__'):
            for name, attr in model.__dict__.items():
                if not name.startswith('_') and name not in ['config']:
                    module_list.append((name, attr))
    
    # ç»Ÿè®¡ç›¸åŒç±»å‹çš„ç»„ä»¶
    component_counts = {}
    
    # ç¬¬ä¸€éï¼šç»Ÿè®¡ç»„ä»¶ç±»å‹
    for name, module in module_list:
        if name == 'layers' and isinstance(module, list) and len(module) > 0:
            # ç‰¹æ®Šå¤„ç†layersåˆ—è¡¨
            layer_type = module[0].__class__.__name__
            component_counts[layer_type] = len(module)
        else:
            module_type = module.__class__.__name__
            component_counts[module_type] = component_counts.get(module_type, 0) + 1
    
    # ç¬¬äºŒéï¼šæ‰“å°ç»„ä»¶ä¿¡æ¯
    printed_components = set()
    
    for name, module in module_list:
        module_type = module.__class__.__name__
        
        # æ£€æµ‹æ˜¯å¦ä¸ºlayersåˆ—è¡¨
        if name == 'layers' and isinstance(module, list):
            # æ‰“å°layersåˆ—è¡¨ä¿¡æ¯
            if len(module) > 0:
                layer_type = module[0].__class__.__name__
                print(f"{prefix}+-- {name}: [{len(module)}ä¸ª{layer_type}]")
                
                # åªæ˜¾ç¤ºç¬¬ä¸€å±‚çš„è¯¦ç»†ç»“æ„ä½œä¸ºç¤ºä¾‹
                if len(module) > 0 and layer_type not in printed_components:
                    print(f"{prefix}    |-- ç¤ºä¾‹ç»“æ„:")
                    print_layer_details("0", module[0], prefix + "    ")
                    printed_components.add(layer_type)
                    
                # å¦‚æœå±‚æ•°è¶…è¿‡1ï¼Œæ˜¾ç¤ºè®¡æ•°ä¿¡æ¯
                if len(module) > 1:
                    print(f"{prefix}    |-- å…¶ä½™{len(module)-1}å±‚ç»“æ„ç›¸åŒ")
        else:
            # æ‰“å°éåˆ—è¡¨ç»„ä»¶
            if component_counts[module_type] > 1 and module_type in printed_components:
                # å·²æ‰“å°è¿‡è¯¥ç±»å‹çš„ç»„ä»¶ï¼Œä¸å†é‡å¤æ‰“å°è¯¦æƒ…
                continue
            else:
                # é¦–æ¬¡æ‰“å°è¯¥ç±»å‹çš„ç»„ä»¶
                if component_counts[module_type] > 1:
                    print(f"{prefix}+-- {name}: {module_type} [å…±{component_counts[module_type]}ä¸ª]")
                else:
                    print(f"{prefix}+-- {name}: {module_type}")
                
                # æ‰“å°ç»„ä»¶çš„è¯¦ç»†ä¿¡æ¯
                if hasattr(module, 'hidden_size'):
                    print(f"{prefix}    |-- å‚æ•°: hidden_size={module.hidden_size}")
                elif hasattr(module, 'dim'):
                    print(f"{prefix}    |-- å‚æ•°: dim={module.dim}")
                
                # æ ‡è®°ä¸ºå·²æ‰“å°
                printed_components.add(module_type)


def print_layer_details(name, module, prefix):
    """æ‰“å°å•ä¸ªå±‚çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ˜¾ç¤ºæ‰€æœ‰å‚æ•°"""
    # æ£€æŸ¥è¯¥æ¨¡å—æ˜¯å¦æœ‰k_cacheå’Œv_cacheå±æ€§
    has_kv_cache = hasattr(module, 'k_cache') and hasattr(module, 'v_cache')
    cache_info = " ğŸ“Œ åŒ…å«KVç¼“å­˜" if has_kv_cache else ""
    
    # æ‰“å°æ¨¡å—åç§°å’Œç±»å‹
    print(f"{prefix}+-- {name}: {module.__class__.__name__}{cache_info}")
    
    # æ‰“å°æ¨¡å—çš„æ‰€æœ‰å‚æ•°ä¿¡æ¯
    module_params = []
    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å‚æ•°
    if hasattr(module, 'config'):
        if hasattr(module.config, 'num_attention_heads'):
            module_params.append(f"num_heads={module.config.num_attention_heads}")
        if hasattr(module.config, 'num_key_value_heads'):
            module_params.append(f"num_kv_heads={module.config.num_key_value_heads}")
        if hasattr(module.config, 'hidden_size'):
            module_params.append(f"hidden_size={module.config.hidden_size}")
        if hasattr(module.config, 'intermediate_size'):
            module_params.append(f"intermediate_size={module.config.intermediate_size}")
        if hasattr(module.config, 'rms_norm_eps'):
            module_params.append(f"rms_norm_eps={module.config.rms_norm_eps}")
        if hasattr(module.config, 'vocab_size'):
            module_params.append(f"vocab_size={module.config.vocab_size}")
    else:
        if hasattr(module, 'num_heads'):
            module_params.append(f"num_heads={module.num_heads}")
        if hasattr(module, 'head_dim'):
            module_params.append(f"head_dim={module.head_dim}")
        if hasattr(module, 'num_kv_heads'):
            module_params.append(f"num_kv_heads={module.num_kv_heads}")
        if hasattr(module, 'scale'):
            module_params.append(f"scale={module.scale:.4f}")
        if hasattr(module, 'hidden_size'):
            module_params.append(f"hidden_size={module.hidden_size}")
        if hasattr(module, 'dim'):
            module_params.append(f"dim={module.dim}")
        if hasattr(module, 'vocab_size'):
            module_params.append(f"vocab_size={module.vocab_size}")
    
    if module_params:
        print(f"{prefix}    |-- å…¨éƒ¨å‚æ•°: {', '.join(module_params)}")
    
    # ç‰¹æ®Šå¤„ç†MockQwen3DecoderLayerç±»å‹çš„æ¨¡å—
    if isinstance(module, MockQwen3DecoderLayer):
        # æ‰“å°DecoderLayerçš„å­æ¨¡å—ç»“æ„
        print(f"{prefix}    |-- å­æ¨¡å—ç»“æ„:")
        print(f"{prefix}        |-- self_attn: MockQwen3Attention ğŸ“Œ åŒ…å«KVç¼“å­˜")
        print(f"{prefix}        |-- mlp: MockMLP")
        print(f"{prefix}        |-- input_layernorm: MockLayerNorm")
        print(f"{prefix}        |-- post_attention_layernorm: MockLayerNorm")


def analyze_kv_cache_usage(model):
    """åˆ†ææ¨¡å‹ä¸­KVç¼“å­˜çš„ä½¿ç”¨æƒ…å†µ"""
    print("\n=== KVç¼“å­˜ä½¿ç”¨åˆ†æ ===")
    layers_with_kv_cache = []
    
    def find_kv_cache_layers(module, path=''):
        # é¦–å…ˆå°è¯•ä½¿ç”¨named_childrenæ–¹æ³•
        try:
            for name, child in module.named_children():
                current_path = f"{path}.{name}" if path else name
                if hasattr(child, 'k_cache') and hasattr(child, 'v_cache'):
                    layers_with_kv_cache.append((current_path, child))
                find_kv_cache_layers(child, current_path)
        except (AttributeError, TypeError):
            # å¦‚æœæ²¡æœ‰named_childrenæ–¹æ³•æˆ–è°ƒç”¨å¤±è´¥ï¼Œå°è¯•ç›´æ¥æ£€æŸ¥å±æ€§
            if hasattr(module, '__dict__'):
                for attr_name, attr_value in module.__dict__.items():
                    if not attr_name.startswith('_') and attr_name not in ['config']:
                        current_path = f"{path}.{attr_name}" if path else attr_name
                        if hasattr(attr_value, 'k_cache') and hasattr(attr_value, 'v_cache'):
                            layers_with_kv_cache.append((current_path, attr_value))
                        # é€’å½’æ£€æŸ¥å­å±æ€§
                        try:
                            find_kv_cache_layers(attr_value, current_path)
                        except:
                            pass
    
    find_kv_cache_layers(model)
    
    if layers_with_kv_cache:
        print(f"å‘ç° {len(layers_with_kv_cache)} ä¸ªä½¿ç”¨KVç¼“å­˜çš„å±‚ï¼š")
        # åªæ˜¾ç¤ºå‰å‡ ä¸ªå’Œåå‡ ä¸ªå±‚çš„è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…è¿‡å¤šé‡å¤
        display_count = 3
        if len(layers_with_kv_cache) <= 2 * display_count:
            # å±‚æ•°é‡è¾ƒå°‘æ—¶å…¨éƒ¨æ˜¾ç¤º
            for path, layer in layers_with_kv_cache:
                print_kv_cache_details(path, layer)
        else:
            # å±‚æ•°é‡è¾ƒå¤šæ—¶åªæ˜¾ç¤ºå‰å‡ ä¸ªå’Œåå‡ ä¸ª
            for path, layer in layers_with_kv_cache[:display_count]:
                print_kv_cache_details(path, layer)
            print(f"... ä¸­é—´ {len(layers_with_kv_cache) - 2*display_count} å±‚çœç•¥ ...")
            for path, layer in layers_with_kv_cache[-display_count:]:
                print_kv_cache_details(path, layer)
    else:
        print("æœªæ‰¾åˆ°ä½¿ç”¨KVç¼“å­˜çš„å±‚")


def print_kv_cache_details(path, layer):
    """æ‰“å°KVç¼“å­˜çš„è¯¦ç»†ä¿¡æ¯"""
    print(f"- {path}: {layer.__class__.__name__}")
    print(f"  - k_cacheå½¢çŠ¶: {layer.k_cache.shape if layer.k_cache.numel() > 0 else 'æœªåˆå§‹åŒ–'}")
    print(f"  - v_cacheå½¢çŠ¶: {layer.v_cache.shape if layer.v_cache.numel() > 0 else 'æœªåˆå§‹åŒ–'}")
    
    # æ‰“å°é¢å¤–çš„ç¼“å­˜ç›¸å…³å‚æ•°
    cache_params = []
    if hasattr(layer, 'num_heads'):
        cache_params.append(f"num_heads={layer.num_heads}")
    if hasattr(layer, 'num_kv_heads'):
        cache_params.append(f"num_kv_heads={layer.num_kv_heads}")
    if hasattr(layer, 'head_dim'):
        cache_params.append(f"head_dim={layer.head_dim}")
    
    if cache_params:
        print(f"  - ç¼“å­˜å‚æ•°: {', '.join(cache_params)}")


def simulate_kv_cache_allocation(model, num_kvcache_blocks, block_size, num_kv_heads, head_dim):
    """æ¨¡æ‹ŸKVç¼“å­˜åˆ†é…è¿‡ç¨‹ï¼Œé€‚ç”¨äºæ¨¡æ‹Ÿç±»å¯¹è±¡"""
    print("\n=== KVç¼“å­˜åˆ†é…æ¨¡æ‹Ÿ ===")
    print("åˆ†é…å‚æ•°:")
    print(f"- KVç¼“å­˜å—æ•°é‡: {num_kvcache_blocks}")
    print(f"- å—å¤§å°: {block_size}")
    print(f"- KVå¤´æ•°: {num_kv_heads}")
    print(f"- å¤´ç»´åº¦: {head_dim}")
    
    # è®¡ç®—æ¯å±‚ç¼“å­˜å¤§å°
    layer_cache_size_mb = (2 * block_size * num_kv_heads * head_dim * 4) / (1024 * 1024)  # å‡è®¾float32
    print(f"- æ¯å±‚ç¼“å­˜å¤§å°: çº¦ {layer_cache_size_mb:.2f} MB")
    
    # æ¨¡æ‹Ÿåˆ†é…è¿‡ç¨‹
    total_allocated_blocks = 0
    layers_with_allocation = 0
    
    def allocate_kv_cache_to_layers(module):
        nonlocal total_allocated_blocks, layers_with_allocation
        
        # æ£€æŸ¥å½“å‰æ¨¡å—æ˜¯å¦ä¸ºDecoderLayer
        if isinstance(module, MockQwen3DecoderLayer) and hasattr(module, 'self_attn'):
            # ä¸ºè¯¥å±‚åˆ†é…KVç¼“å­˜
            layers_with_allocation += 1
            total_allocated_blocks += 2  # æ¯ä¸ªå±‚åˆ†é…2ä¸ªå—ï¼ˆKå’ŒVï¼‰
    
    # å¼€å§‹åˆ†é…
    # ç”±äºæˆ‘ä»¬çš„æ¨¡å‹æ˜¯ç®€åŒ–çš„ï¼Œç›´æ¥éå†layersåˆ—è¡¨
    if hasattr(model, 'layers') and isinstance(model.layers, list):
        print(f"  æ­£åœ¨ä¸º {len(model.layers)} ä¸ªDecoderLayerå±‚åˆ†é…KVç¼“å­˜...")
        for i, layer in enumerate(model.layers):
            # è®¾ç½®å±‚ç´¢å¼•ä»¥ä¾¿äºè·Ÿè¸ª
            layer.layer_index = i
            allocate_kv_cache_to_layers(layer)
        print(f"  æ‰€æœ‰å±‚KVç¼“å­˜åˆ†é…å®Œæˆ")
    
    # æ‰“å°åˆ†é…ç»“æœ
    print(f"\nåˆ†é…ç»“æœ:")
    print(f"- åˆ†é…KVç¼“å­˜çš„å±‚æ•°: {layers_with_allocation}")
    print(f"- æ€»å…±åˆ†é…çš„å—æ•°: {total_allocated_blocks}")
    print(f"- æ€»ç¼“å­˜å¤§å°: çº¦ {total_allocated_blocks * block_size * num_kv_heads * head_dim * 4 / (1024 * 1024):.2f} MB")
    print(f"- å‰©ä½™å¯ç”¨å—æ•°: {num_kvcache_blocks - total_allocated_blocks}")
    print(f"\nKVç¼“å­˜åˆ†å¸ƒä½ç½®:")
    print("- æ‰€æœ‰Qwen3DecoderLayerå±‚çš„self_attnç»„ä»¶éƒ½åŒ…å«ç‹¬ç«‹çš„KVç¼“å­˜")
    print("- æ¯å±‚ç¼“å­˜åŒ…å«Kå’ŒVä¸¤ä¸ªéƒ¨åˆ†ï¼Œåˆ†åˆ«ç”¨äºå­˜å‚¨é”®å’Œå€¼çš„å†å²ä¿¡æ¯")
    print("- ç¼“å­˜é‡‡ç”¨å—ç»“æ„ç®¡ç†ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•å’Œé‡Šæ”¾")


def main():
    """ä¸»å‡½æ•°ï¼šåŠ è½½æ¨¡å‹å¹¶å±•ç¤ºç»“æ„"""
    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„é…ç½®å¯¹è±¡
    class MockConfig:
        def __init__(self):
            self.model = "/opt/llm/Qwen3-0.6B/"
            self.tensor_parallel_size = 1
            self.hf_config = Qwen3Config(
                vocab_size=151936,
                hidden_size=4096,
                num_hidden_layers=32,
                num_attention_heads=32,
                num_key_value_heads=8,
                intermediate_size=14336,
                hidden_act="silu",
                max_position_embeddings=32768,
                rms_norm_eps=1e-6,
                tie_word_embeddings=False,
                torch_dtype=torch.float16,
                rope_theta=1000000,
                head_dim=128
            )
            self.gpu_memory_utilization = 0.9
            self.eos = 151643  # Qwen3çš„eos token id
    
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = MockConfig()
    
    print("=== Qwen3æ¨¡å‹ç»“æ„ä¸KVç¼“å­˜ä½¿ç”¨åˆ†æ ===")
    print(f"æ¨¡å‹åç§°: {config.model}")
    print(f"å¼ é‡å¹¶è¡Œå¤§å°: {config.tensor_parallel_size}")
    print(f"éšè—å±‚æ•°é‡: {config.hf_config.num_hidden_layers}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {config.hf_config.num_attention_heads}")
    print(f"KVå¤´æ•°: {config.hf_config.num_key_value_heads}")
    print(f"éšè—å±‚å¤§å°: {config.hf_config.hidden_size}")
    
    # åˆ›å»ºQwen3æ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
    print("\n=== æ¨¡å‹ç»“æ„ ===")
    hf_config = config.hf_config
    
    # åˆ›å»ºä¸€ä¸ªQwen3Modelå®ä¾‹
    model = Qwen3Model(hf_config)
    
    # æ‰“å°æ¨¡å‹ç»“æ„
    print_model_structure(model)
    
    # åˆ†æKVç¼“å­˜ä½¿ç”¨æƒ…å†µ
    analyze_kv_cache_usage(model)
    
    # æ¨¡æ‹ŸKVç¼“å­˜åˆ†é…
    # å‡è®¾çš„å‚æ•°å€¼ï¼Œä»…ç”¨äºæ¼”ç¤º
    num_kvcache_blocks = 100
    block_size = 16
    num_kv_heads = hf_config.num_key_value_heads // config.tensor_parallel_size
    head_dim = hf_config.head_dim
    
    simulate_kv_cache_allocation(model, num_kvcache_blocks, block_size, num_kv_heads, head_dim)
    
    # å†æ¬¡åˆ†æKVç¼“å­˜ä½¿ç”¨æƒ…å†µï¼Œç¡®è®¤åˆ†é…
    analyze_kv_cache_usage(model)
    
    print("\n=== KVç¼“å­˜ç»‘å®šæœºåˆ¶è¯´æ˜ ===")
    print("1. KVç¼“å­˜åœ¨model_runner.pyçš„allocate_kv_cacheæ–¹æ³•ä¸­ç»Ÿä¸€åˆ†é…")
    print("2. åˆ†é…çš„ç¼“å­˜å½¢çŠ¶ä¸ºï¼š[2, å±‚æ•°, å—æ•°, å—å¤§å°, å¤´æ•°, å¤´ç»´åº¦]")
    print("3. å…¶ä¸­ç¬¬ä¸€ä¸ªç»´åº¦çš„0è¡¨ç¤ºkeyç¼“å­˜ï¼Œ1è¡¨ç¤ºvalueç¼“å­˜")
    print("4. é€šè¿‡éå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—ï¼Œå°†æ¯ä¸€å±‚çš„k_cacheå’Œv_cacheç»‘å®šåˆ°å¯¹åº”çš„ç¼“å­˜ä½ç½®")
    print("5. åœ¨å®é™…æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ–°è®¡ç®—çš„kå’Œvä¼šé€šè¿‡store_kvcacheå‡½æ•°å­˜å‚¨åˆ°å…¨å±€ç¼“å­˜ä¸­")
    print("6. è¿™ç§è®¾è®¡ä½¿å¾—å¤šä¸ªåºåˆ—å¯ä»¥å…±äº«KVç¼“å­˜ï¼Œæé«˜æ˜¾å­˜åˆ©ç”¨ç‡å’Œæ¨ç†æ•ˆç‡")


if __name__ == "__main__":
    main()